---
layout: post
title: Self-RAG - Learning to Retrieve, Generate, and Critique through Self-Reflection
subtitle: Self-RAG
category: RAG
tags: [LLM, RAG]
permalink: /2025/02/17/Self_RAG/
css : /css/ForYouTubeByHyun.css
bigimg: 
  - "/img/Image/BigImages/carmel.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/monterey.jpg" : "Monterey, CA (2016)"
  - "/img/Image/BigImages/stanford_dish.jpg" : "Stanford Dish, CA (2016)"
  - "/img/Image/BigImages/marian_beach_in_sanfran.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/carmel2.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/marina.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/sanfrancisco.jpg" : "San Francisco, CA (2016)"
  
---

This post is a brief summary about the paper that I read for my study and curiosity, so I shortly arrange the content of the paper, titled [Self-RAG: Learaning to Retrieve, Generate, and Critique through Self-Reflection (Asai et al. ICLR 2024)](https://openreview.net/forum?id=hSyW5go0v8), that I read and studied. 

{% include MathJax.html %}


![Asai et al. ICLR 2024](/img/Image/NaturalLanguageProcessing/Papers/RAG/2025-02-17-Self_RAG/Self-RAG_01.png)

![Asai et al. ICLR 2024](/img/Image/NaturalLanguageProcessing/Papers/RAG/2025-02-17-Self_RAG/Self-RAG_02.png)


For detailed experiment and explanation, refer to the paper, titled [Self-RAG: Learaning to Retrieve, Generate, and Critique through Self-Reflection (Asai et al. ICLR 2024)](https://openreview.net/forum?id=hSyW5go0v8)

<div class="alert alert-info" role="alert"><i class="fa fa-info-circle"></i> <b>Note(Abstract): </b>
Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its generations using special tokens, called {\it reflection} tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning, and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models. Our code and trained models are available at https://selfrag.github.io/
</div>

<div class="alert alert-success" role="alert"><i class="fa fa-paperclip fa-lg"></i> <b>Download URL: </b><br>
  <a href="https://openreview.net/forum?id=hSyW5go0v8">The paper: VisText: A Benchmark for Semantically Rich Chart Captioning(Tang et al., ICLR 2024)</a></div>

# Reference 

- Paper 
  - [ICLR Version: Self-RAG: Learaning to Retrieve, Generate, and Critique through Self-Reflection (Asai et al. ICLR 2024)](https://openreview.net/forum?id=hSyW5go0v8)
  
- How to use html for alert
  - [how to use icon](http://idratherbewriting.com/documentation-theme-jekyll/mydoc_icons.html)
 
- How to use MathJax 
  - [MathJax basic tutorial and quick reference in StackExchange](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference)

