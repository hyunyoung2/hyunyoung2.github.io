---
layout: post
title: Search-R1 - Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning
subtitle: Reinforcement Learning for RAG
category: RAG
tags: [LLM, RAG]
permalink: /2025/04/24/SearchR1/
css : /css/ForYouTubeByHyun.css
bigimg: 
  - "/img/Image/BigImages/carmel.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/monterey.jpg" : "Monterey, CA (2016)"
  - "/img/Image/BigImages/stanford_dish.jpg" : "Stanford Dish, CA (2016)"
  - "/img/Image/BigImages/marian_beach_in_sanfran.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/carmel2.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/marina.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/sanfrancisco.jpg" : "San Francisco, CA (2016)"
  
---

This post is a brief summary about the paper that I read for my study and curiosity, so I shortly arrange the content of the paper, titled [Search-R1 - Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning (Jin et al. arXiv 2025)](https://arxiv.org/abs/2503.09516), that I read and studied. 

{% include MathJax.html %}


The following is the example of prompt for Reinforcement Learning on Searh-R1.

![Jin et al. arXiv 2025](/img/Image/NaturalLanguageProcessing/Papers/RAG/2025-04-24-SearchR1/SearchR1.png)


For detailed experiment and explanation, refer to the paper, titled [Search-R1 - Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning (Jin et al. arXiv 2025)](https://arxiv.org/abs/2503.09516)

<div class="alert alert-info" role="alert"><i class="fa fa-info-circle"></i> <b>Note(Abstract): </b>
Efficiently acquiring external knowledge and up-to-date information is essential for effective reasoning and text generation in large language models (LLMs). Prompting advanced LLMs with reasoning capabilities to use search engines during inference is often suboptimal, as the LLM might not fully possess the capability on how to interact optimally with the search engine. This paper introduces Search-R1, an extension of reinforcement learning (RL) for reasoning frameworks where the LLM learns to autonomously generate (multiple) search queries during step-by-step reasoning with real-time retrieval. Search-R1 optimizes LLM reasoning trajectories with multi-turn search interactions, leveraging retrieved token masking for stable RL training and a simple outcome-based reward function. Experiments on seven question-answering datasets show that Search-R1 improves performance by 41% (Qwen2.5-7B) and 20% (Qwen2.5-3B) over various RAG baselines under the same setting. This paper further provides empirical insights into RL optimization methods, LLM choices, and response length dynamics in retrieval-augmented reasoning. The code and model checkpoints are available at [this https URL](https://github.com/PeterGriffinJin/Search-R1).
</div>

<div class="alert alert-success" role="alert"><i class="fa fa-paperclip fa-lg"></i> <b>Download URL: </b><br>
  <a href="https://arxiv.org/abs/2503.09516">The paper: Search-R1: Training LLLMs to Reason and Leverage Search Engines with Reinforcement Learning (Jin et al. arXiv 2024)</a></div>

# Reference 

- Paper 
  - [arXiv Version: Search-R1 - Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning (Jin et al. arXiv 2025)](https://arxiv.org/abs/2503.09516)
 
- For your information
  - [A Visual Guide to LLM Agents](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-llm-agents)
  
- How to use html for alert
  - [how to use icon](http://idratherbewriting.com/documentation-theme-jekyll/mydoc_icons.html)
 
- How to use MathJax 
  - [MathJax basic tutorial and quick reference in StackExchange](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference)

