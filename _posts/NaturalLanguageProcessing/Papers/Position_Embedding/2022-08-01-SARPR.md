---
layout: post
title: Self-Attention with Relative Position Representations
subtitle: Title of paper - Self-Attention with Relative Position Representations
category: Transformer - Position Embedding
tags: [Transformer, position]
permalink: /2022/08/01/SARPR/
css : /css/ForYouTubeByHyun.css
bigimg: 
  - "/img/Image/BigImages/carmel.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/monterey.jpg" : "Monterey, CA (2016)"
  - "/img/Image/BigImages/stanford_dish.jpg" : "Stanford Dish, CA (2016)"
  - "/img/Image/BigImages/marian_beach_in_sanfran.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/carmel2.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/marina.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/sanfrancisco.jpg" : "San Francisco, CA (2016)"
  
---

This post is a brief summary about the paper that I read for my study and curiosity, so I shortly arrange the content of the paper, titled [Self-Attention with Relative Position Representations, Shaw et al., NAACL 2018](https://aclanthology.org/N18-2074/), that I read and studied. 

{% include MathJax.html %}


The original paper([Vaswani et al. NIPS 2017](https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)) for transformer architecture uses position encoding in input layer. On the contrary, this paper proposes the relative position embedding in the internal structure of transformer. 


how they incorporate the relative position embedding in the transformer structure is as follows:

For value embedding section,


$$z_i = \sum_{j=1}^n \alpha_{ij}(x_j W^V + a_{ij}^V)$$


For key embedding section, 

$$e_{ij} = \frac{x_i W^Q (x_j W^K + a_{ij}^K)^T}{\sqrt d_z}$$


As you can see Figure 1 below, they used the relative position encoding on key and value vectors.

![Shaw et al. NAACL 2018](/img/Image/NaturalLanguageProcessing/Papers/Position_Embedding/2022-08-01-SARPR/SARPR_Figure.png)


For detailed explanation of the above equations, refer to the paper, titled [Self-Attention with Relative Position Representations, Shaw et al., NAACL 2018](https://aclanthology.org/N18-2074/) 

Since they extend self-attention to take account of the pairwise relationships between input elements, they model the input as a labeled, directed, and fully connected graph. 

They don't consider all of the relative positions in a input sequence, so they learn representations for each relative position within a clipping distancs K.

They hypothesized the precise relative position information is not useful beyond a certain distance clipped. 

     
<div class="alert alert-success" role="alert"><i class="fa fa-paperclip fa-lg"></i> <b>Download URL: </b><br>
  <a href="https://aclanthology.org/N18-2074/">The paper: Self-Attention with Relative Position Representations (Shaw et al., NAACL 2018)</a>
</div>

# Reference 

- Paper 
  - [arXiv Version: Self-Attention with Relative Position Representations (Shaw et al., arXiv 2018)](https://arxiv.org/abs/1803.02155)
  - [NAACL Version: Self-Attention with Relative Position Representations (Shaw et al., NAACL 2018)](https://aclanthology.org/N18-2074/)
  
- How to use html for alert
  - [how to use icon](http://idratherbewriting.com/documentation-theme-jekyll/mydoc_icons.html)
 
- How to use MathJax 
  - [MathJax basic tutorial and quick reference in StackExchange](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference)
