---
layout: post
title: Self-Attention with Relative Position Representations
subtitle: Title of paper - Self-Attention with Relative Position Representations
category: Transformers - Position Embedding
tags: [Trainsformer, position]
permalink: /2022/08/01/SARPR/
css : /css/ForYouTubeByHyun.css
bigimg: 
  - "/img/Image/BigImages/carmel.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/monterey.jpg" : "Monterey, CA (2016)"
  - "/img/Image/BigImages/stanford_dish.jpg" : "Stanford Dish, CA (2016)"
  - "/img/Image/BigImages/marian_beach_in_sanfran.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/carmel2.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/marina.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/sanfrancisco.jpg" : "San Francisco, CA (2016)"
  
---

This article is brief summary about the paper that I read for study and curiosity, so I shortly arranges the content of the paper, titled [Self-Attention with Relative Position Representations, Shaw et al., NAACL 2018](https://aclanthology.org/N18-2074/), that I read and studied. 

{% include MathJax.html %}


The original paper([Vaswani et al. NIPS 2017](https://papers.nips.cc/paper/2019/hash/9d63484abb477c97640154d40595a3bb-Abstract.html)) use positin encoddeing in input layer. On the contrary, this paper proposes the relative position embedidng in the internal structure of transformer. 


how they incorporate relative position embedding in the transformer structure is as follows:

For key embedding section,


$$\z_i = \sum_{j=1}^n \alphs_{ij}(x_j W^V + a_{ij}^V$$


For value embeddin section, 

$$\e_ij = \frac{x_i W^Q (x_j W^K + a_{ij}^K)^T }{\sqrt(d_z)}$$


As you can see figure 1 below, they used the relative position encoding on key and value vectors.

![Shaw et al. NAACL 2018](~~~)


For detailed expalanation of the above equations, refer to the paper, the paper, titled [Self-Attention with Relative Position Representations](https://aclanthology.org/N18-2074/) 

Since they extend self-attention to take account of the pairwise relationships between input elements, they model the input as a labeled, directed, fully connected graph. 

They don't consider all of the relative positions in a input sequence, so they learn representations for each relative position within a clipping distancs K.

They hypothesized the precise relative position information is not usefult beyon a certain distance with the clipping concept. 

     
<div class="alert alert-success" role="alert"><i class="fa fa-paperclip fa-lg"></i> <b>Download URL: </b><br>
  <a href="https://aclanthology.org/N18-2074/">The paper: Self-Attention with Relative Position Representations (Shaw et al., NAACL 2018)</a>
</div>

# Reference 

- Paper 
  - [arXiv Version: Self-Attention with Relative Position Representations (Shaw et al., arXiv 2018)](https://arxiv.org/abs/1803.02155)
  - [NAACL Version: Self-Attention with Relative Position Representations (Shaw et al., NAACL 2018)](https://aclanthology.org/N18-2074/)
  
- How to use html for alert
  - [how to use icon](http://idratherbewriting.com/documentation-theme-jekyll/mydoc_icons.html)
 
- How to use MathJax 
  - [MathJax basic tutorial and quick reference in StackExchange](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference)
