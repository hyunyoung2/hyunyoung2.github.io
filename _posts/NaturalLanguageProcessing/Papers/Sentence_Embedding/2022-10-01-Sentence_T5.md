---
layout: post
title: \[Sentence-T5\] Scalable Sentence Encoders from Pre-trained Text-to-Text Models
subtitle: Sentence-T5
category: Transformer - Setence Embedding
tags: [Sentence embedding, transformer]
permalink: /2022/10/01/Sentence_T5/
css : /css/ForYouTubeByHyun.css
bigimg: 
  - "/img/Image/BigImages/carmel.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/monterey.jpg" : "Monterey, CA (2016)"
  - "/img/Image/BigImages/stanford_dish.jpg" : "Stanford Dish, CA (2016)"
  - "/img/Image/BigImages/marian_beach_in_sanfran.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/carmel2.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/marina.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/sanfrancisco.jpg" : "San Francisco, CA (2016)"
  
---

This post is a brief summary about the paper that I read for my study and curiosity, so I shortly arrange the content of the paper, titled [Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models (Ni et al., ACL 2022)](https://aclanthology.org/2022.findings-acl.146/), that I read and studied. 

{% include MathJax.html %}

They proposed how to represent sentence embeddinf with T5 model known as pretrained model as text-to-text format. 

They call the proposed method ST5 which stands for Sentence-T5.

As you can see figure below, they empirically tried to compare Senten-T5 with BERT-based sentence embedding on sentence task

![Ni et al. ACL 2022](/img/Image/NaturalLanguageProcessing/Papers/Sentence_Embedding/2022-10-01-Sentence_T5/Sentence_T5.png)

They proposed two structures to represent sentence embedding, ST5 Encoder-only model and ST5 EncDec model with T5 model. 

ST5 Encoder-only utilizes the encoder part from encoder-decoder of T5 model, the ST5 EncDec utilizes encoder-decoder structure of T5 model.

To sum up, ST5 Encoder only with mean pooling and first token or ST5 EncDec with first token on decoder is used for sentence embedding.  

You can see the detailed empirical analysis and experiemtn in the paper, titled [Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models (Ni et al., ACL 2022)](https://aclanthology.org/2022.findings-acl.146/)

For detailed experiment and explanation, refer to the paper, titled [Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models (Ni et al., ACL 2022)](https://aclanthology.org/2022.findings-acl.146/)

<div class="alert alert-success" role="alert"><i class="fa fa-paperclip fa-lg"></i> <b>Download URL: </b><br>
  <a href="https://aclanthology.org/2022.findings-acl.146/">The paper: Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models (Ni et al., ACL 2022)</a>
</div>

# Reference 

- Paper 
  - [ACL2022 Version: Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models (Ni et al., ACL 2022)](https://aclanthology.org/2022.findings-acl.146/)
  - [ArXiv Version: Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models (Ni et al., ACL 2022)](https://arxiv.org/abs/2108.08877)
  
- How to use html for alert
  - [how to use icon](http://idratherbewriting.com/documentation-theme-jekyll/mydoc_icons.html)
 
- How to use MathJax 
  - [MathJax basic tutorial and quick reference in StackExchange](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference)
