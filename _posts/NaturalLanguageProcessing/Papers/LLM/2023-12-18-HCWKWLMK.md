---
layout: post
title: How Can We Know What Language Modles Know?
subtitle: Language Model Knowledge Analysis
category: LLM_Knowledge
tags: [LLM, Knowledge, Prompt]
permalink: /2023/10/14/ITI/
css : /css/ForYouTubeByHyun.css
bigimg: 
  - "/img/Image/BigImages/carmel.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/monterey.jpg" : "Monterey, CA (2016)"
  - "/img/Image/BigImages/stanford_dish.jpg" : "Stanford Dish, CA (2016)"
  - "/img/Image/BigImages/marian_beach_in_sanfran.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/carmel2.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/marina.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/sanfrancisco.jpg" : "San Francisco, CA (2016)"
  
---

This post is a brief summary about the paper that I read for my study and curiosity, so I shortly arrange the content of the paper, titled [How Can We Know What Lnaguage Models Know? (Jiang et al., TACL 2020)](https://aclanthology.org/2020.tacl-1.28/), that I read and studied. 

{% include MathJax.html %}

They guage the knowledge of langauge with prompt as follows:

But they focused on the prompt style based on the paper titeled as [Language models as knowledge base? (Petroni et al. EMNLP 2019)](https://aclanthology.org/D19-1250/)

The style of prompt is retained, but the quality of prompt is improved.

![Jiang et al. TACL 2020](/img/Image/NaturalLanguageProcessing/Papers/LLM/2023-12-18-HCWKWLMK/HCWKWLMK_01.png)

They propose the method of prompt genration like both Mining-based generation and Paraphrasing-base generation. 

- The mining-based generation uses the cooccurence words between subject and object, and the other mining-based generation is the dependecy-based prompts.

- The paraphrasing-based generation denotes the original prompt into other semantically similar or identical expressions, so they used back-translation for paraphrasing method.

In order to verify the knowledge of language model, they generate the improved prompts and also test to exctract th knowledge using ensembling at the test time for the knowledeg of language model.

For the verified method with the generated prompts at test tiem, they propsoe the three methods as follows:

 - Top 1 Prompt Selection
   
 - Rang-based Enemble

 - Optimized Ensemble

You can see the detailed empirical analysis and experiemtn in the paper, titled [How Can We Know What Lnaguage Models Know? (Jiang et al., TACL 2020)](https://aclanthology.org/2020.tacl-1.28/)

For detailed experiment and explanation, refer to the paper, titled [How Can We Know What Lnaguage Models Know? (Jiang et al., TACL 2020)](https://aclanthology.org/2020.tacl-1.28/)

<div class="alert alert-success" role="alert"><i class="fa fa-paperclip fa-lg"></i> <b>Download URL: </b><br>
  <a href="https://aclanthology.org/2020.tacl-1.28/">The paper: How Can We Know What Language Models Know? (Jiang et al., TACL 2020)</div>

# Reference 

- Paper 
  - [TACL Version: How Can We Know What Lnaguage Models Know? (Jiang et al., TACL 2020)](https://aclanthology.org/2020.tacl-1.28/)
  - [ArXiv Version: How Can We Know What Language Models Know? (Jiang et al., arXiv 2020)](https://arxiv.org/abs/1911.12543)
  
- How to use html for alert
  - [how to use icon](http://idratherbewriting.com/documentation-theme-jekyll/mydoc_icons.html)
 
- How to use MathJax 
  - [MathJax basic tutorial and quick reference in StackExchange](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference)
