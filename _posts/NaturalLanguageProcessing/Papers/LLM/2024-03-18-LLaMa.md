---
layout: post
title: LLaMa - Open and Efficient Foundation Language Models
subtitle: LLaMa
category: LLM
tags: [LLM]
permalink: /2024/03/18/LLaMa/
css : /css/ForYouTubeByHyun.css
bigimg: 
  - "/img/Image/BigImages/carmel.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/monterey.jpg" : "Monterey, CA (2016)"
  - "/img/Image/BigImages/stanford_dish.jpg" : "Stanford Dish, CA (2016)"
  - "/img/Image/BigImages/marian_beach_in_sanfran.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/carmel2.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/marina.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/sanfrancisco.jpg" : "San Francisco, CA (2016)"
  
---

This post is a brief summary about the paper that I read for my study and curiosity, so I shortly arrange the content of the paper, titled [LLaMa: Open and Efficient Foundation Language Models (Touvron et al., arXiv 2023)](https://arxiv.org/abs/2302.13971), that I read and studied. 

{% include MathJax.html %}

LLaMa is a collection of fundation language models ranging from 7B to 65B parameters.

They focused on the situation like that the best performances are not acheived by the largest models but by smaller models trained on more data for a given compute budget. 

Furthermore, They showed that the [prior research by Hoffmann et al. 2022](https://arxiv.org/abs/2203.15556) is to determine how to best scale the dataset and model size for a particular training compute budget saying [The Hoffmann et al. 2022](https://arxiv.org/abs/2203.15556)  disregarding the infference budget. 

They think of the best case for serving model that given a target level of performance, the preferred model is not the fastest to trian but the fatest at inference. 

So, their work focuses on training a series of language models that achieve the best possible perforemance at various inference budgets, by training on more tokens that whtat is typically used. 

For detailed experiment and explanation, refer to the paper, titled [LLaMa: Open and Efficient Foundation Language Models (Touvron et al., arXiv 2023)](https://arxiv.org/abs/2302.13971)

<div class="alert alert-success" role="alert"><i class="fa fa-paperclip fa-lg"></i> <b>Download URL: </b><br>
  <a href="https://arxiv.org/abs/2302.13971">The paper: LLaMa: Open and Efficient Foundation Language Models (Touvron et al., arXiv 2023)</a></div>

# Reference 

- Paper 
  - [ArXiv Version: LLaMa: Open and Efficient Foundation Language Models (Touvron et al., arXiv 2023)](https://arxiv.org/abs/2302.13971)
  
- How to use html for alert
  - [how to use icon](http://idratherbewriting.com/documentation-theme-jekyll/mydoc_icons.html)
 
- How to use MathJax 
  - [MathJax basic tutorial and quick reference in StackExchange](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference)

