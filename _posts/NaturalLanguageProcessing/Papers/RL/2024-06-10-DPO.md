---
layout: post
title: Direct Preference Optimization - Your Language Model is Secretly a Reward Model
subtitle: DPO
category: LLM
tags: [LLM, Feedback, Reward]
permalink: /2024/04/01/DPO/
css : /css/ForYouTubeByHyun.css
bigimg: 
  - "/img/Image/BigImages/carmel.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/monterey.jpg" : "Monterey, CA (2016)"
  - "/img/Image/BigImages/stanford_dish.jpg" : "Stanford Dish, CA (2016)"
  - "/img/Image/BigImages/marian_beach_in_sanfran.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/carmel2.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/marina.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/sanfrancisco.jpg" : "San Francisco, CA (2016)"
  
---

This post is a brief summary about the paper that I read for my study and curiosity, so I shortly arrange the content of the paper, titled [Direct Preference Optimization: Your Langauge Model is Secretly a Reward Model (Rafailov et al., arXiv 2023)](https://arxiv.org/abs/2305.18290), that I read and studied. 

{% include MathJax.html %}

They provide DPO, a simple training paradigm for trainig language models from preferences without reinforcemente learning.

Rather than forccing the preference learning into a standard RL setting in order to use off-the-shelf RL algorithm, DPO identifies a mapping between language model policies and rward functions that enables training a language model to satisfy haman preferences directly, with a simple cross-enttropy loss not using RL or loss of generality.

They said that for distribution shift, DPO is better than PPO validatingcomp the experiment to use a different distribution on summarization task.

![Rafailov et al., arXiv 2023](/img/Image/NaturalLanguageProcessing/Papers/RL/2024-06-10-DPO/DPO_01.png)


For detailed experiment and explanation, refer to the paper, titled [Direct Preference Optimization: Your Langauge Model is Secretly a Reward Model (Rafailov et al., arXiv 2023)](https://arxiv.org/abs/2305.18290)

<div class="alert alert-success" role="alert"><i class="fa fa-paperclip fa-lg"></i> <b>Download URL: </b><br>
  <a href="https://arxiv.org/abs/2305.18290">The paper: Direct Preference Optimizatio: Your Language Model is Secretly a Reward Model (RaFailov et al., arXiv 2023)</a></div>

# Reference 

- Paper 
  - [ArXiv Version: Direct Preference Optimization: Your Langauge Model is Secretly a Reward Model (Rafailov et al., arXiv 2023)](https://arxiv.org/abs/2305.18290)
 
- HuggingFace
  - [DPOTrainer](https://huggingface.co/docs/trl/main/en/dpo_trainer)
  - [Fine-tuned Llama 2 with DPO](https://huggingface.co/blog/dpo-trl)
  
- How to use html for alert
  - [how to use icon](http://idratherbewriting.com/documentation-theme-jekyll/mydoc_icons.html)
 
- How to use MathJax 
  - [MathJax basic tutorial and quick reference in StackExchange](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference)

