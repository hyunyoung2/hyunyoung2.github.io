---
layout: post
title: ORPO - Monolithic Preference Optimization without preference Model
subtitle: ORPO
category: LLM
tags: [LLM, Feedback, Reward, RL]
permalink: /2024/07/09/ORPO/
css : /css/ForYouTubeByHyun.css
bigimg: 
  - "/img/Image/BigImages/carmel.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/monterey.jpg" : "Monterey, CA (2016)"
  - "/img/Image/BigImages/stanford_dish.jpg" : "Stanford Dish, CA (2016)"
  - "/img/Image/BigImages/marian_beach_in_sanfran.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/carmel2.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/marina.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/sanfrancisco.jpg" : "San Francisco, CA (2016)"
  
---

This post is a brief summary about the paper that I read for my study and curiosity, so I shortly arrange the content of the paper, titled [ORPO: Monolithic Preference Optimization without Reference Model (Hong et al., arXiv 2024)](https://arxiv.org/abs/2403.07691), that I read and studied. 

{% include MathJax.html %}




![Hong et al., arXiv 2024](//img/Image/NaturalLanguageProcessing/Papers/RL/2024-07-09-ORPO/ORPO_01.png)

$$
 loss_{ORPO}= E_{(x, y_w, y_l)[loss_{SFT} + \lambda * loss_{OR}]
$$

They perform RL with a penalty with expectiona $\beta KL(\pi, \rho)$ using the following loss:

$$
 loss_{OR} =  - log(\sigma\log(\dfrac{odds_{\theta}(y_l|x}{odds_{\theta)(y_w|x)}))
$$


For detailed experiment and explanation, refer to the paper, titled [ORPO: Monolithic Preference Optimization without Reference Model (Hong et al., arXiv 2024)](https://arxiv.org/abs/2403.07691)

<div class="alert alert-success" role="alert"><i class="fa fa-paperclip fa-lg"></i> <b>Download URL: </b><br>
  <a href="https://arxiv.org/abs/2403.07691">The paper: ORPO: Monolithic Preference Optimization without Reference Model (Hong et al., arXiv 2024)</a></div>

# Reference 

- Paper 
  - [ArXiv Version: ORPO: Monolithic Preference Optimization without Reference Model (Hong et al., arXiv 2024)](https://arxiv.org/abs/2403.07691)

- HuggingFace TRL 
  - [TRL - Transformer Reinforcement Learning](https://huggingface.co/docs/trl/index)
  - [CPOconfig in Trainer of TRL](https://huggingface.co/docs/trl/main/en/trainer)
  
- How to use html for alert
  - [how to use icon](http://idratherbewriting.com/documentation-theme-jekyll/mydoc_icons.html)
 
- How to use MathJax 
  - [MathJax basic tutorial and quick reference in StackExchange](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference)

