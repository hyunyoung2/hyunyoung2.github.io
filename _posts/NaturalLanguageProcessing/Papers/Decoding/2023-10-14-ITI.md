---
layout: post
title: (DoLA) DECODING BY CONTRASTING LAYERS IMPROVES FACTUALITY IN LARGE LANGUAGE MODELS
subtitle: DoLA
category: Decoding
tags: [LLM, Deconding, Factuality]
permalink: /2023/10/14/DoLA/
css : /css/ForYouTubeByHyun.css
bigimg: 
  - "/img/Image/BigImages/carmel.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/monterey.jpg" : "Monterey, CA (2016)"
  - "/img/Image/BigImages/stanford_dish.jpg" : "Stanford Dish, CA (2016)"
  - "/img/Image/BigImages/marian_beach_in_sanfran.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/carmel2.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/marina.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/sanfrancisco.jpg" : "San Francisco, CA (2016)"
  
---

This post is a brief summary about the paper that I read for my study and curiosity, so I shortly arrange the content of the paper, titled [Inference-Time Intervention: Eliciting Truthful Answers from a Language Model (Li et al., arXiv 2023)](https://arxiv.org/abs/2306.03341), that I read and studied. 

{% include MathJax.html %}

They propose adapting the truthfulness of LLMs using attetnion head. 

![Li et al. ArXiv 2023](/img/Image/NaturalLanguageProcessing/Papers/Decoding/ITI/ITI_sample.png)


ITI (Inference-time Intervention) is an alternative form of MHA, where:

![Li et al. ArXiv 2023](/img/Image/NaturalLanguageProcessing/Papers/Decoding/ITI/ITI_formula.png)

But, ITI is supervised learning, they used the activation editing. 

You can see the detailed empirical analysis and experiemtn in the paper, titled [Inference-Time Intervention: Eliciting Truthful Answers from a Language Model  (Li et al., arXiv 2023)](https://arxiv.org/abs/2306.03341)

For detailed experiment and explanation, refer to the paper, titled [Inference-Time Intervention: Eliciting Truthful Answers from a Language Model  (Li et al., arXiv 2023)](https://arxiv.org/abs/2306.03341)

<div class="alert alert-success" role="alert"><i class="fa fa-paperclip fa-lg"></i> <b>Download URL: </b><br>
  <a href="https://arxiv.org/abs/2306.03341">The paper: Inference-Time Intervention: Eliciting Truthful Answers from a Language Model  (Li et al., arXiv 2023)</div>

# Reference 

- Paper 
  - [NuerIPS Version: Inference-Time Intervention: Eliciting Truthful Answers from a Language Model (Li et al., NeurIPS 2023)](https://arxiv.org/abs/2306.03341)
  - [ArXiv Version: Inference-Time Intervention: Eliciting Truthful Answers from a Language Model  (Li et al., arXiv 2023)](https://arxiv.org/abs/2306.03341)
  
- How to use html for alert
  - [how to use icon](http://idratherbewriting.com/documentation-theme-jekyll/mydoc_icons.html)
 
- How to use MathJax 
  - [MathJax basic tutorial and quick reference in StackExchange](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference)
