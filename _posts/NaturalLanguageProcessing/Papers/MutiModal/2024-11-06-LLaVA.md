---
layout: post
title: Visual Instruction Tuning
subtitle: LLaVA
category: Multi-modal
tags: [LLM, VLLM]
permalink: /2024/11/06/LLaVA/
css : /css/ForYouTubeByHyun.css
bigimg: 
  - "/img/Image/BigImages/carmel.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/monterey.jpg" : "Monterey, CA (2016)"
  - "/img/Image/BigImages/stanford_dish.jpg" : "Stanford Dish, CA (2016)"
  - "/img/Image/BigImages/marian_beach_in_sanfran.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/carmel2.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/marina.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/sanfrancisco.jpg" : "San Francisco, CA (2016)"
  
---

This post is a brief summary about the paper that I read for my study and curiosity, so I shortly arrange the content of the paper, titled [Visual Instruction Tuning (Liu et al., arXiv 2023)](https://arxiv.org/abs/2304.08485), that I read and studied. 

{% include MathJax.html %}

The primary goal is to effectively leverage the capabilities of both the pre-trained LLM and visual model. The network archtecture is illustrated in Figure 1. 
![Liu et al. arXiv 2023](/img/Image/NaturalLanguageProcessing/Papers/multi-modal/2024-11-28-LLaVa/LLaVa1.png)

For viaul token $$H_v$$, they use linear projection. 

![Liu et al. arXiv 2023](/img/Image/NaturalLanguageProcessing/Papers/multi-modal/2024-11-28-LLaVa/LLaVa2.png)


For Instruction $${X^t}_instruction$, it is comprised of the follwoing:

![Liu et al. arXiv 2023](/img/Image/NaturalLanguageProcessing/Papers/multi-modal/2024-11-28-LLaVa/LLaVa3.png)


For detailed experiment and explanation, refer to the paper, titled [Visual Instruction Tuning (Liu et al., arXiv 2023)](https://arxiv.org/abs/2304.08485)

<div class="alert alert-info" role="alert"><i class="fa fa-info-circle"></i> <b>Note(Abstract): </b>
Instruction tuning large language models (LLMs) using machine-generated instruction-following data has been shown to improve zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. We present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and an LLM for generalpurpose visual and language understanding. To facilitate future research on visual instruction following, we construct two evaluation benchmarks with diverse and challenging application-oriented tasks. Our experiments show that LLaVA demonstrates impressive multimodal chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions.
</div>

<div class="alert alert-success" role="alert"><i class="fa fa-paperclip fa-lg"></i> <b>Download URL: </b><br>
  <a href="https://arxiv.org/abs/2304.08485">The paper: Visual Instruction Tuning (Liu et al., arXiv 2023)</a></div>

# Reference 

- Paper 
  - [ArXiv Version: Visual Instruction Tuning (Liu et al., arXiv 2023)](https://arxiv.org/abs/2304.08485)
  - [NeurIPS: Visual instruction Tunin (Liu et al., NeurIPS 2023)](https://papers.nips.cc/paper_files/paper/2023/hash/6dcf277ea32ce3288914faf369fe6de0-Abstract-Conference.html)
 
- For Your Information
  - [Lil'Log's Gneralized Visual Language MOdels](https://lilianweng.github.io/posts/2022-06-09-vlm/)

- How to use html for alert
  - [how to use icon](http://idratherbewriting.com/documentation-theme-jekyll/mydoc_icons.html)
 
- How to use MathJax 
  - [MathJax basic tutorial and quick reference in StackExchange](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference)

