---
layout: post
title: ChartQA - A Benchmark for Question Answering about Charts with Visual and Logical Reasoning
subtitle: ChartQA
category: Multi-modal
tags: [LLM, VLLM]
permalink: /2024/12/24/ChartQA/
css : /css/ForYouTubeByHyun.css
bigimg: 
  - "/img/Image/BigImages/carmel.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/monterey.jpg" : "Monterey, CA (2016)"
  - "/img/Image/BigImages/stanford_dish.jpg" : "Stanford Dish, CA (2016)"
  - "/img/Image/BigImages/marian_beach_in_sanfran.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/carmel2.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/marina.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/sanfrancisco.jpg" : "San Francisco, CA (2016)"
  
---

This post is a brief summary about the paper that I read for my study and curiosity, so I shortly arrange the content of the paper, titled [ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning (Masry et al., arXiv 2022)](https://arxiv.org/abs/2203.10244), that I read and studied. 

{% include MathJax.html %}


![Masry et al. arXiv 2022](/img/Image/NaturalLanguageProcessing/Papers/multi-modal/2024-12-24-ChartQA/ChartQA_01.png)


For detailed experiment and explanation, refer to the paper, titled [ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning (Masry et al., arXiv 2022)](https://arxiv.org/abs/2203.10244)

<div class="alert alert-info" role="alert"><i class="fa fa-info-circle"></i> <b>Note(Abstract): </b>
Charts are very popular for analyzing data. When exploring charts, people often ask a variety of complex reasoning questions that involve several logical and arithmetic operations. They also commonly refer to visual features of a chart in their questions. However, most existing datasets do not focus on such complex reasoning questions as their questions are template-based and answers come from a fixed-vocabulary. In this work, we present a large-scale benchmark covering 9.6K human-written questions as well as 23.1K questions generated from human-written chart summaries. To address the unique challenges in our benchmark involving visual and logical reasoning over charts, we present two transformer-based models that combine visual features and the data table of the chart in a unified way to answer questions. While our models achieve the state-of-the-art results on the previous datasets as well as on our benchmark, the evaluation also reveals several challenges in answering complex reasoning questions.
</div>

<div class="alert alert-success" role="alert"><i class="fa fa-paperclip fa-lg"></i> <b>Download URL: </b><br>
  <a href="https://arxiv.org/abs/2203.10244">The paper: ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning(Masry et al., arXiv 2022)</a></div>

# Reference 

- Paper 
  - [ArXiv Version: ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning (Masry et al., arXiv 2022)](https://arxiv.org/abs/2203.10244)
  
- For Your Information
  - [Lil'Log's Gneralized Visual Language MOdels](https://lilianweng.github.io/posts/2022-06-09-vlm/)

- How to use html for alert
  - [how to use icon](http://idratherbewriting.com/documentation-theme-jekyll/mydoc_icons.html)
 
- How to use MathJax 
  - [MathJax basic tutorial and quick reference in StackExchange](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference)

