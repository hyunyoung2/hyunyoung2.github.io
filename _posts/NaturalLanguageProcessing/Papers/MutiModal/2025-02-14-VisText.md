---
layout: post
title: VisText - A Benchmark for Semantically Rich Chart Captioning
subtitle: ChartQA
category: Multi-modal
tags: [LLM, VLLM]
permalink: /2025/02/14/VisText/
css : /css/ForYouTubeByHyun.css
bigimg: 
  - "/img/Image/BigImages/carmel.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/monterey.jpg" : "Monterey, CA (2016)"
  - "/img/Image/BigImages/stanford_dish.jpg" : "Stanford Dish, CA (2016)"
  - "/img/Image/BigImages/marian_beach_in_sanfran.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/carmel2.jpg" : "Carmel-by-the-Sea, CA (2016)"
  - "/img/Image/BigImages/marina.jpg" : "MRINA of San Francisco, CA (2016)"
  - "/img/Image/BigImages/sanfrancisco.jpg" : "San Francisco, CA (2016)"
  
---

This post is a brief summary about the paper that I read for my study and curiosity, so I shortly arrange the content of the paper, titled [VisText: A Benchmark for Semantically Rich Chart Captioning(Tang et al., arXiv 2023)](https://arxiv.org/abs/2307.05356), that I read and studied. 

{% include MathJax.html %}


![Tang et al. arXiv 2023](/img/Image/NaturalLanguageProcessing/Papers/multi-modal/2025-02-14-VisText/VisText_01.png)


For detailed experiment and explanation, refer to the paper, titled [VisText: A Bechmark for Semantically Rich Chart Captioning(Tang et al., arXiv 2023)](https://arxiv.org/abs/2307.05356)

<div class="alert alert-info" role="alert"><i class="fa fa-info-circle"></i> <b>Note(Abstract): </b>
Captions that describe or explain charts help improve recall and comprehension of the depicted data and provide a more accessible medium for people with visual disabilities. However, current approaches for automatically generating such captions struggle to articulate the perceptual or cognitive features that are the hallmark of charts (e.g., complex trends and patterns). In response, we introduce VisText: a dataset of 12,441 pairs of charts and captions that describe the charts' construction, report key statistics, and identify perceptual and cognitive phenomena. In VisText, a chart is available as three representations: a rasterized image, a backing data table, and a scene graph -- a hierarchical representation of a chart's visual elements akin to a web page's Document Object Model (DOM). To evaluate the impact of VisText, we fine-tune state-of-the-art language models on our chart captioning task and apply prefix-tuning to produce captions that vary the semantic content they convey. Our models generate coherent, semantically rich captions and perform on par with state-of-the-art chart captioning models across machine translation and text generation metrics. Through qualitative analysis, we identify six broad categories of errors that our models make that can inform future work.
</div>

<div class="alert alert-success" role="alert"><i class="fa fa-paperclip fa-lg"></i> <b>Download URL: </b><br>
  <a href="https://arxiv.org/abs/2307.05356">The paper: VisText: A Benchmark for Semantically Rich Chart Captioning(Tang et al., arXiv 2023)</a></div>

# Reference 

- Paper 
  - [ArXiv Version: VisText: A Benchmark for Semantically Rich Chart Captioning (Tang et al. arXiv 2023) ](https://arxiv.org/abs/2307.05356)
  
- For Your Information
  - [Lil'Log's Gneralized Visual Language MOdels](https://lilianweng.github.io/posts/2022-06-09-vlm/)

- How to use html for alert
  - [how to use icon](http://idratherbewriting.com/documentation-theme-jekyll/mydoc_icons.html)
 
- How to use MathJax 
  - [MathJax basic tutorial and quick reference in StackExchange](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference)

